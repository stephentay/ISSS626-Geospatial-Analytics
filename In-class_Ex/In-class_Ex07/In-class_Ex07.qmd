---
title: "In-class Ex 7"
subtitle: ""
author: "Stephen Tay"
date: "14 Oct 2024"
date-modified:  "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  warning: false
  freeze: true 
---

# 1. Overview
In this in-class exercise, we build a hedonic pricing model for condominiums using **Geographically Weighted Regression (GWR)** to account for spatial non-stationarity, where relationships between variables differ across geographic locations. This exercise builds on hands-on exercise 7, with additional emphasis on other R packages relevant to GWR

```{r}
pacman::p_load(olsrr, ggstatsplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary, performance, see, sfdep)
```

# 2. Importing & Transforming Data

We will import and work with two datasets:

- **URA's 2014 master plan subzone boundary**
- **Condominium resale prices 2015**

::: panel-tabset
## 1. Importing master plan subzone
We use `st_transform()` to assign the correct ESPG code 3414.
```{r}
mpsz <- st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL") %>%
  st_transform(3414)
```
## 2. Import condo resale prices dataset
We import the condo resale prices dataset using `read_csv()`.
```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
head(condo_resale)
```
## 3. Convert aspatial dataset to sf object
Since the CSV file contains latitude and longitude coordinates, but the coordinate system is unknown, we make an informed guess and assign the closest geographic coordinate system—EPSG 4326. (You could verify the accuracy by plotting the data to ensure the spatial points align with real-world locations). If confirmed, we transform the data to the projected coordinate system EPSG 3414.
```{r}
condo_resale_sf = condo_resale %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), 
           crs=4326) %>% # use the geo coordinate system of the lat/long in the csv file
  st_transform(crs=3414)
head(condo_resale_sf)
```

:::

# 3. Building the Hedonic Price Model
The following steps are taken to build the hedonic price model.

## 3.1 Correlation Analysis
To prevent multicollinearity, it is essential to examine relationships between variables to identify those with high correlations. While immediate removal isn’t necessary, these variables should be closely monitored during the multicollinearity test later.

As an alternative to the `corrplot` package, the `ggcorrmat()` function from the `ggstatsplot` package can also be used for correlation analysis.

```{r}
#| fig-width: 12
#| fig-height: 10
ggcorrmat(condo_resale[, 5:23])
```

## 3.2 Initial Hedonic Pricing Model Using MLR
We begin by building an initial MLR model with all variables deemed relevant for predicting the selling price. We will assess the model in the next step.
```{r}
condo_mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD + 
                  LEASEHOLD_99YR, 
                data=condo_resale_sf)
summary(condo_mlr)
```

## 3.3 Model Assessment (using `olsrr`)
In model assessment, we evaluate the model’s p-value and R² to determine overall significance and explanatory power. Next, we assess individual variables to identify any that are non-significant, as these should be removed to enhance the model’s robustness.

The `olsrr` package is used to generate a comprehensive, tidy report for assessing the MLR model. The summary report indicates that the model is statistically significant (p < 0.05) and explains 64.7% of the variance in the dependent variable. However, since some independent variables are not statistically significant, they should be excluded from the final model.

```{r}
ols_regress(condo_mlr)
```

## 3.4 Checking Multicollinearity
We use the following code to compute the Variance Inflation Factor (VIF) to help us identify multicollinearity. A VIF between 5 and 10 indicates moderate multicollinearity and requires monitoring, while a VIF above 10 suggests severe multicollinearity, warranting variable elimination. 

As all VIF values are below 10, no variables need to be removed.

```{r}
ols_vif_tol(condo_mlr)
```

## 3.5 Variable Selection
We use the `ols_step_forward_p()` function to perform stepwise forward selection. Although there are other criteria that could guide the selection process, we prioritise the p-value to ensure that all variables included in the final model are statistically significant.

We can visualise the stepwise forward selection process using the `plot()` function, which displays the incremental changes in Adjusted R², AIC, and RMSE throughout the selection process.
```{r}
condo_fw_mlr <- ols_step_forward_p(condo_mlr,
                                   p_val = 0.05,
                                   details = FALSE)
```

```{r}
#| fig-width: 12
#| fig-height: 10
plot(condo_fw_mlr)
```

## 3.6 Visualising Model Parameters
The following method allows us to visualise all the model parameters.
```{r}
ggcoefstats(condo_mlr, sort = "ascending")
```

## 3.7 Testing for Non-linearity
It is important to test the assumption of linearity and additivity in the relationship between the dependent and independent variables. The figure shows that most data points are scattered around the zero line, indicating that the relationships between the dependent and independent variables are linear.
```{r}
ols_plot_resid_fit(condo_fw_mlr$model)
```

## 3.8 Testing Normality of Residuals
We use `ols_plot_resid_hist()` and `ols_test_normality()` to check the normality of the residuals.
```{r}
ols_plot_resid_hist(condo_fw_mlr$model)
```

```{r}
ols_test_normality(condo_fw_mlr$model)
```
## 3.9 Testing for Spatial Autocorrelation
The hedonic model incorporates geographically referenced attributes, making it essential to visualize the residuals. To test for spatial autocorrelation, we must convert the condo_resale.sf dataset from an sf object to a SpatialPointsDataFrame.

First, we export the residuals from the hedonic pricing model and save them as a data frame.
```{r}
mlr_output <- as.data.frame(condo_fw_mlr$model$residuals) %>%
  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)
```

Next, we will join the newly created dataframe with condo_resale_sf object.
```{r}
condo_resale_sf <- cbind(condo_resale_sf,
                         mlr_output$FW_MLR_RES) %>%
  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)
```

## 3.10 Map Plot of Residuals
We plot the residuals on a map to identify areas of overestimation and underestimation. Visible clusters of over- or under-estimated prices may indicate the presence of spatial autocorrelation.

```{r}
tmap_mode("view")
#tmap_options(check.and.fix = TRUE) -- add this code here to fix any layers with problematic lines/polygons.

tm_shape(mpsz) +
  tmap_options(check.and.fix = TRUE) + # add this line here to explicitly fix problematic polygons in this specific layer.
  tm_polygons(alpha = 0.4) +
  tm_shape(condo_resale_sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

## 3.11 Spatial Stationarity Test
We compute the distance-based weight matrix using `sfdep` package.

```{r}
condo_resale_sf <- condo_resale_sf %>%
  mutate(nb = st_knn(geometry, k = 6, longlat = FALSE),
         wt = st_weights(nb, style = "W"),
         .before = 1)
```

To confirm the presence of spatial autocorrelation, we perform the Moran’s I test.

-	H₀: The residuals are randomly distributed (spatially stationary).
-	H₁: The residuals exhibit spatial dependence (spatially non-stationary).

We conduct a Global Moran’s I permutation test to determine whether spatial autocorrelation exists in the residuals.

The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.
Since the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution.
```{r}
global_moran_perm(condo_resale_sf$MLR_RES,
                  condo_resale_sf$nb,
                  condo_resale_sf$wt,
                  alternative = "two.sided",
                  nsim = 99)
```
# 4. Building GWR Model
In this section, we will build the hedonic pricing models using GWR.

## 4.1 Fixed Bandwidth GWR Model
In the code below, the `bw.gwr()` function from the `GWmodel` package is used to determine the optimal fixed bandwidth for the model. Setting the `adaptive` argument to `FALSE` specifies that a fixed bandwidth will be used.

There are two methods to define the stopping rule: the cross-validation (CV) approach and the AIC corrected (AICc) approach. We define the stopping rule using `approach` agreement.
```{r}
bw_fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE +
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale_sf,
                   approach="CV",
                   kernel = "gaussian",
                   adaptive = FALSE,
                   longlat = FALSE)
```

We use the code chunk below to build the fixed bandwidth GWR model.
There are observable improvements in R2 and the AICc. (Note: AICc is robust for small dataset.)
```{r}
gwr_fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE +
                         PROX_CBD + PROX_CHILDCARE + 
                         PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                         PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                         NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale_sf, 
                       bw=bw_fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
gwr_fixed
```

## 4.2 Adaptive Bandwidth GWR Model
The following code is similar to the one used for computing fixed bandwidth, except that the `adaptive` argument is set to `TRUE`.
The recommended number of data points to use is 30.
```{r}
bw_adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE +
                     PROX_CBD + PROX_CHILDCARE + 
                     PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                     PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale_sf,
                   approach="CV",
                   kernel = "gaussian",
                   adaptive = TRUE,
                   longlat = FALSE)
```
We use the code chunk below to build the adaptive bandwidth GWR model.
The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.
```{r}
gwr_adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE +
                            PROX_CBD + PROX_CHILDCARE + 
                            PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + 
                            PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + 
                            PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                         data=condo_resale_sf, 
                         bw=bw_adaptive,
                         kernel = 'gaussian', 
                         adaptive=TRUE,
                         longlat = FALSE)
gwr_adaptive
```

## 4.3 Visualising SDF Fields
To visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.
```{r}
gwr_adaptive_output <- as.data.frame(gwr_adaptive$SDF) %>%
  select(-c(2:15))

gwr_sf_adaptive <- cbind(condo_resale_sf,
                         gwr_adaptive_output)
glimpse(gwr_sf_adaptive)
```

::: panel-tabset
### Local R2
The code chunks below is used to create an interactive point symbol map of local R2
```{r}
tmap_mode("view")
tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

### Coefficient estimates
```{r}
tmap_options(check.and.fix = TRUE)
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

### Local R2 in Central Region
```{r}
tm_shape(mpsz[mpsz$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(gwr_sf_adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

:::



