---
title: "In-class Ex 6"
subtitle: ""
author: "Stephen Tay"
date: "30 Sep 2024"
date-modified:  "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  warning: false
  freeze: true 
---

# Overview
In this exercise, we will perform an **Emerging Hot Spot Analysis**, a spatio-temporal method used to identify and describe the evolution of hot spots and cold spots over time. The analysis involves four main steps:

-	Building a space-time cube.
-	Calculating the Getis-Ord Gi* statistic for each bin using an FDR correction (there are some differences in calculating Gi* when we are using time series data).
-	Evaluating hot and cold spot trends with the Mann-Kendall (MK) trend test.
-	Categorising each study area location based on the trend’s z-score and p-value, along with the hot spot z-score and p-value.

The `plotly` package enables the creation of interactive plots, allowing you to explore and monitor trends over time.
The `Kendall` package is used for conducting Mann-Kendall trend tests.
```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, Kendall)
```

# 1. Importing & Transforming Data
Two datasets will be used in this in-class exercise:

- **Hunan County Boundary Layer**: A geospatial dataset in ESRI shapefile format.
- **Hunan_GDPPC.csv**: A CSV file containing GDPPC indicator for Hunan counties, from 2005 to 2021.

::: panel-tabset
## Hunan shapefile
In this in-class exercise, there’s no need to convert the Geographical Coordinate System to a projected coordinate system.
```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan") 
glimpse(hunan)
```
## Hunan's GDPPC
```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
glimpse(GDPPC)
```
:::

# 2. Creating a Time Series Cube
`spacetime()` function of the `sfdep` package is used to create an spatio-temporal cube. In using the spacetime cube, **the space entity (e.g. administrative boundary or the hexagon) must remain constant over time**; only the attribute in the space entity changes over time.
It is useful to verify that your object is a space-time cube object using the `is_spacetime_cube()` function.

Note that your time field must be an integer (it cannot be a real number, e.g. you may have to drop the time values in order to make it an integer time value.)
```{r}
GDPPC_st <- spacetime(GDPPC, # attribute datafile
                      hunan, # geospatial datefile
                      .loc_col = "County", # spatial field
                      .time_col = "Year" # time field which must be in integer
                      )
is_spacetime_cube(GDPPC_st)
```

# 3. Computing Gi*
We are computing local Gi* statistics which includes self (Gi statistic does not include self).
When computing Gi* over time, we need to activate the geometry context using `activate("geometry")`.

The code below identifies neighbours and derives inverse distance weights.

We use `set_nbs()` and `set_wts()` to copy the "nb" and "wt" columns to each time slice. This dataset now has neighbours and weights for each time-slice.

Do not rearrange the rows as row order is very important in the analysis.
```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(
    st_contiguity(geometry)),
    wt = st_inverse_distance(nb, # we are using the inverse distance weight
                             geometry,
                             scale = 1,
                             alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>% # we fix them as row order is very important 
  set_wts("wt")
```

Next, we calculate the local Gi* for each location by grouping the data by year and using the `local_gstar_perm()` function from the `sfdep` package. Then, we use `unnest()` to expand the `gi_star` column in the newly created gi_star data frame.
```{r}
gi_star <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

# 4. Mann-Kendall Test
The Mann-Kendall (MK) test is a statistical method used to confirm the presence of a monotonic trend in a time series. The hypotheses are as follows:

- H0: No monotonic trend.
- H1: A monotonic trend (increasing or decreasing) is present.

We reject the null hypothesis if the p-value is smaller than the chosen alpha level. The tau value, which ranges from -1 to 1, indicates the direction and strength of the trend: 

- -1 represents a perfectly decreasing trend
- 1 signifies a perfectly increasing trend.

To perform an MK test, at least 12 time periods of data are required.

::: panel-tabset
## Changsha example
Using the computed local Gi* statistics, we can assess the trend at each location with the MK test. 

Let’s take Changsha County as an example and plot its Gi* statistics from 2005 to 2021.
```{r}
cbg <- gi_star %>%
  ungroup() %>%
  filter(County == "Changsha") %>%
  select(County, Year, gi_star)
```

```{r}
ggplot(data = cbg, aes(x = Year, y = gi_star)) +
  geom_line() +
  theme_light()
```

## Changsha's interactive Mann-Kendall plot
We can create an interactive MK plot using `ggplotly()` function from the `plotly` package.
```{r}
p <- ggplot(data = cbg, aes(x = Year, y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

## Changsha's MK test report
In the results, `sl` represents the p-value. Since the p-value is less than 0.05 and tau is greater than 0, we reject the null hypothesis, indicating a slight upward trend.

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```
## Dataframe of MK test for each location
We can perform the Mann-Kendall test for all locations simultaneously at once using `group_by(County)`.

The resulting dataframe reveals that some counties exhibit statistically significant trends, while others do not. Additionally, some show positive trends, while others display a decline.
```{r}
ehsa <- gi_star %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
head(ehsa)
```

## Sorting of dataframe
We could sort the dataframe to show significant emerging hot/cold spots.
```{r}
emerging <- ehsa %>%
  arrange(sl, abs(tau)) %>%
  slice(1:10)
head(emerging)
```
:::

# 5. Emerging Hotspot Analysis

::: panel-tabset
## `emerging_hotspot_analysis()`
Lastly, we perform Emerging Hot Spot Analysis (EHSA) using the `emerging_hotspot_analysis()` function from the `sfdep` package. For the `x` argument, we pass the spacetime object (e.g., GDPPC_st), and for `.var`, we specify the variable of interest (e.g., GDPPC). The `k` argument, which sets the number of time lags, defaults to 1. In the `nsim` argument, we define the number of simulations to run; setting it to 99, for example, results in 100 simulations.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st,
  .var = "GDPPC",
  k = 1,
  nsim = 99
)
```

## Visualising distribution of EHSA classes
We can visualise the distribution of EHSA classes using a bar chart.

Note that this is a basic plot and does not account for statistical significance. If significance were considered, non-statistically significant areas would fall into the “no pattern detected” category.
```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

## EHSA choropleth map
Before plotting the geographic distribution of EHSA classes, we need to join the `ehsa` dataframe with the `hunan` sf dataframe.

Next, we use the `tmap` functions to create a categorical choropleth map.
```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa, by = join_by(County == location))
```

```{r}
ehsa_sig <- hunan_ehsa %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  tm_shape(ehsa_sig) +
  tm_fill("classification") +
  tm_borders(alpha = 0.4)
```
:::
